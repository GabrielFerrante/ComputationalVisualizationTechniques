{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be1bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from ipywidgets import AppLayout\n",
    "from numpy import asarray\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from efficientnet.tfkeras import EfficientNetB0, preprocess_input\n",
    "from shap.plots._image import colors\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import shap\n",
    "import cv2\n",
    "import warnings\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec43196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DadosEmbeddingsNormalizados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebbdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scatter = px.scatter(df,\n",
    "    x=\"x\", y=\"y\",\n",
    "    marginal_x=\"histogram\", height=500, width=600,\n",
    "    color=\"ClasseOriginal\", hover_data=['ImageName', df.index], title=\"Embeddings das imagens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ab576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mypath = f\"./valid/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "imagensOrdenadas = sorted(onlyfiles)\n",
    "\n",
    "#USAR PARA REDIMENSIONAR AS IMAGENS PARA 224X224\n",
    "\n",
    "def redimensionar(imagem):\n",
    "  imagem = cv2.resize(imagem, (224, 224),interpolation=cv2.INTER_CUBIC)\n",
    "  return imagem\n",
    "\n",
    "for img in onlyfiles:\n",
    "  imagem = cv2.imread(f'./valid/{img}')\n",
    "  if imagem.shape[0] != 224 or imagem.shape[1] != 224:\n",
    "        novaImagem = redimensionar(imagem)\n",
    "        cv2.imwrite(mypath+\"/\"+img, novaImagem)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700cf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB0(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44db9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image(shap_values, pixel_values=None, labels=None, width=20, aspect=0.2, hspace=0.2, labelpad=None, show=True):\n",
    "    \"\"\" Plots SHAP values for image inputs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap_values : [numpy.array]\n",
    "        List of arrays of SHAP values. Each array has the shap (# samples x width x height x channels), and the\n",
    "        length of the list is equal to the number of model outputs that are being explained.\n",
    "    pixel_values : numpy.array\n",
    "        Matrix of pixel values (# samples x width x height x channels) for each image. It should be the same\n",
    "        shape as each array in the shap_values list of arrays.\n",
    "    labels : list\n",
    "        List of names for each of the model outputs that are being explained. This list should be the same length\n",
    "        as the shap_values list.\n",
    "    width : float\n",
    "        The width of the produced matplotlib plot.\n",
    "    labelpad : float\n",
    "        How much padding to use around the model output labels.\n",
    "    show : bool\n",
    "        Whether matplotlib.pyplot.show() is called before returning. Setting this to False allows the plot\n",
    "        to be customized further after it has been created.\n",
    "    \"\"\"\n",
    "\n",
    "    # support passing an explanation object\n",
    "    if str(type(shap_values)).endswith(\"Explanation'>\"):\n",
    "        shap_exp = shap_values\n",
    "        # feature_names = [shap_exp.feature_names]\n",
    "        # ind = 0\n",
    "        if len(shap_exp.output_dims) == 1:\n",
    "            shap_values = [shap_exp.values[..., i] for i in range(shap_exp.values.shape[-1])]\n",
    "        elif len(shap_exp.output_dims) == 0:\n",
    "            shap_values = shap_exp.values\n",
    "        else:\n",
    "            raise Exception(\"Number of outputs needs to have support added!! (probably a simple fix)\")\n",
    "        if pixel_values is None:\n",
    "            pixel_values = shap_exp.data\n",
    "        if labels is None:\n",
    "            labels = shap_exp.output_names\n",
    "\n",
    "    multi_output = True\n",
    "    if not isinstance(shap_values, list):\n",
    "        multi_output = False\n",
    "        shap_values = [shap_values]\n",
    "\n",
    "    if len(shap_values[0].shape) == 3:\n",
    "        shap_values = [v.reshape(1, *v.shape) for v in shap_values]\n",
    "        pixel_values = pixel_values.reshape(1, *pixel_values.shape)\n",
    "\n",
    "    # make sure labels\n",
    "    if labels is not None:\n",
    "        labels = np.array(labels)\n",
    "        if labels.shape[0] != shap_values[0].shape[0] and labels.shape[0] == len(shap_values):\n",
    "            labels = np.tile(np.array([labels]), shap_values[0].shape[0])\n",
    "        assert labels.shape[0] == shap_values[0].shape[0], \"Labels must have same row count as shap_values arrays!\"\n",
    "        if multi_output:\n",
    "            assert labels.shape[1] == len(shap_values), \"Labels must have a column for each output in shap_values!\"\n",
    "        else:\n",
    "            assert len(labels.shape) == 1, \"Labels must be a vector for single output shap_values.\"\n",
    "\n",
    "    label_kwargs = {} if labelpad is None else {'pad': labelpad}\n",
    "\n",
    "    # plot our explanations\n",
    "    x = pixel_values\n",
    "    fig_size = np.array([3 * (len(shap_values) + 1), 2.5 * (x.shape[0] + 1)])\n",
    "    if fig_size[0] > width:\n",
    "        fig_size *= width / fig_size[0]\n",
    "    fig, axes = pl.subplots(nrows=x.shape[0], ncols=len(shap_values) + 1, figsize=fig_size)\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = axes.reshape(1,axes.size)\n",
    "    for row in range(x.shape[0]):\n",
    "        x_curr = x[row].copy()\n",
    "\n",
    "        # make sure we have a 2D array for grayscale\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 1:\n",
    "            x_curr = x_curr.reshape(x_curr.shape[:2])\n",
    "        if x_curr.max() > 1:\n",
    "            x_curr /= 255.\n",
    "\n",
    "        # get a grayscale version of the image\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 3:\n",
    "            x_curr_gray = (0.2989 * x_curr[:,:,0] + 0.5870 * x_curr[:,:,1] + 0.1140 * x_curr[:,:,2]) # rgb to gray\n",
    "            x_curr_disp = x_curr\n",
    "        elif len(x_curr.shape) == 3:\n",
    "            x_curr_gray = x_curr.mean(2)\n",
    "\n",
    "            # for non-RGB multi-channel data we show an RGB image where each of the three channels is a scaled k-mean center\n",
    "            flat_vals = x_curr.reshape([x_curr.shape[0]*x_curr.shape[1], x_curr.shape[2]]).T\n",
    "            flat_vals = (flat_vals.T - flat_vals.mean(1)).T\n",
    "            means = kmeans(flat_vals, 3, round_values=False).data.T.reshape([x_curr.shape[0], x_curr.shape[1], 3])\n",
    "            x_curr_disp = (means - np.percentile(means, 0.5, (0,1))) / (np.percentile(means, 99.5, (0,1)) - np.percentile(means, 1, (0,1)))\n",
    "            x_curr_disp[x_curr_disp > 1] = 1\n",
    "            x_curr_disp[x_curr_disp < 0] = 0\n",
    "        else:\n",
    "            x_curr_gray = x_curr\n",
    "            x_curr_disp = x_curr\n",
    "\n",
    "        axes[row,0].imshow(x_curr_disp, cmap=pl.get_cmap('gray'))\n",
    "        axes[row,0].axis('off')\n",
    "        if len(shap_values[0][row].shape) == 2:\n",
    "            abs_vals = np.stack([np.abs(shap_values[i]) for i in range(len(shap_values))], 0).flatten()\n",
    "        else:\n",
    "            abs_vals = np.stack([np.abs(shap_values[i].sum(-1)) for i in range(len(shap_values))], 0).flatten()\n",
    "        max_val = np.nanpercentile(abs_vals, 99.9)\n",
    "        for i in range(len(shap_values)):\n",
    "            if labels is not None:\n",
    "                axes[row,i+1].set_title(labels[row,i], **label_kwargs)\n",
    "            sv = shap_values[i][row] if len(shap_values[i][row].shape) == 2 else shap_values[i][row].sum(-1)\n",
    "            axes[row,i+1].imshow(x_curr_gray, cmap=pl.get_cmap('gray'), alpha=0.15, extent=(-1, sv.shape[1], sv.shape[0], -1))\n",
    "            im = axes[row,i+1].imshow(sv, cmap=colors.red_transparent_blue, vmin=-max_val, vmax=max_val)\n",
    "            axes[row,i+1].axis('off')\n",
    "    if hspace == 'auto':\n",
    "        fig.tight_layout()\n",
    "    else:\n",
    "        fig.subplots_adjust(hspace=hspace)\n",
    "    cb = fig.colorbar(im, ax=np.ravel(axes).tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=fig_size[0]/aspect)\n",
    "    cb.outline.set_visible(False)\n",
    "    if show:\n",
    "        pl.savefig('imagemExplicada.jpg', bbox_inches='tight')\n",
    "        #pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5bec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformImageForSHAP(): \n",
    "    arrayImagens = []\n",
    "    i = 0\n",
    "    for img in onlyfiles:\n",
    "      imagem = cv2.imread(f'./valid/{img}')\n",
    "      arrayImagens.append(np.array(imagem.astype('float32')))\n",
    "      i = i + 1\n",
    "    return np.array(arrayImagens)\n",
    "\n",
    "def XAI(dado):\n",
    "\n",
    "    X = TransformImageForSHAP()\n",
    "\n",
    "    url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "    with open(shap.datasets.cache(url)) as file:\n",
    "        class_names = [v[1] for v in json.load(file).values()]\n",
    "\n",
    "    def f(x):\n",
    "        tmp = x.copy()\n",
    "        preprocess_input(tmp)\n",
    "        return model(tmp)\n",
    "\n",
    "\n",
    "    masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n",
    "\n",
    "    explainer = shap.Explainer(f, masker, output_names=class_names)\n",
    "\n",
    "\n",
    "    if int(dado) in df.index:\n",
    "        nomeDaImagem = df.iloc[[int(dado)]][\"ImageName\"].values\n",
    "        if str(nomeDaImagem[0])+'.jpg' in onlyfiles:\n",
    "            indexQuery = onlyfiles.index(str(nomeDaImagem[0])+'.jpg')\n",
    "    else:\n",
    "        print(\"index não existente !\")\n",
    "        \n",
    "    shap_values = explainer(X[indexQuery:indexQuery+1], max_evals=100, batch_size=50, outputs=shap.Explanation.argsort.flip[:2])\n",
    "    image(shap_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0995819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72c8a8d6bbb4f9cad4e4654b2b3ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'customdata': array([['bear_000000000285', 0],…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wid_query = {\n",
    "    \"query\": widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Digite o index da imagem\",\n",
    "        description=\"\",\n",
    "        disabled=False),\n",
    "    \"button\": widgets.Button(\n",
    "        description='Buscar',\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        icon='search', # (FontAwesome names without the `fa-` prefix)\n",
    "        tooltip='Search for a country'),\n",
    "    \"output\": widgets.Image(format='jpg')}\n",
    "\n",
    "def on_query(button):\n",
    "    dado = wid_query[\"query\"].value\n",
    "    XAI(dado)\n",
    "    file = open(\"imagemExplicada.jpg\", \"rb\")\n",
    "    image = file.read()\n",
    "    wid_query[\"output\"].value = image\n",
    "    wid_query[\"output\"].height = \"500\"\n",
    "    wid_query[\"output\"].width = \"500\"\n",
    "    \n",
    "    \n",
    "# Eventos\n",
    "wid_query[\"button\"].on_click(on_query)\n",
    "\n",
    "\n",
    "AppLayout(\n",
    "    height=\"700px\",\n",
    "    header= None,\n",
    "    left_sidebar= widgets.HBox([go.FigureWidget(scatter)]),\n",
    "    center= None,\n",
    "    right_sidebar= widgets.VBox([widgets.Label(value=\"Entre com um index de uma imagem entre 0 à 498 para visualizar as características de classificação\"),widgets.HBox([wid_query[\"query\"], wid_query[\"button\"]]), wid_query[\"output\"]]),\n",
    "    footer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db9af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
